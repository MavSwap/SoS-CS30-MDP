{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd2af4f9-3312-43e8-ae9c-9cab2701e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in /opt/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pygame\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b38f15-a670-4bde-a774-f6b31a07e752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode 0: Total Reward = -97.44, Epsilon = 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hd/nth13llx7673wbk87t31zck00000gn/T/ipykernel_76881/1626295881.py:129: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  states = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10: Total Reward = -69.42, Epsilon = 0.967\n",
      "Episode 20: Total Reward = -81.69, Epsilon = 0.939\n",
      "Episode 30: Total Reward = -69.82, Epsilon = 0.911\n",
      "Episode 40: Total Reward = -73.25, Epsilon = 0.884\n",
      "Episode 50: Total Reward = -87.22, Epsilon = 0.858\n",
      "Episode 60: Total Reward = -90.85, Epsilon = 0.833\n",
      "Episode 70: Total Reward = -81.35, Epsilon = 0.808\n",
      "Episode 80: Total Reward = -68.88, Epsilon = 0.784\n",
      "Episode 90: Total Reward = -81.45, Epsilon = 0.761\n",
      "Episode 100: Total Reward = -69.76, Epsilon = 0.738\n",
      "Episode 110: Total Reward = -70.05, Epsilon = 0.716\n",
      "Episode 120: Total Reward = -68.70, Epsilon = 0.695\n",
      "Episode 130: Total Reward = -66.99, Epsilon = 0.675\n",
      "Episode 140: Total Reward = -89.64, Epsilon = 0.655\n",
      "Episode 150: Total Reward = -84.11, Epsilon = 0.635\n",
      "Episode 160: Total Reward = -98.08, Epsilon = 0.616\n",
      "Episode 170: Total Reward = -73.41, Epsilon = 0.598\n",
      "Episode 180: Total Reward = -88.72, Epsilon = 0.581\n",
      "Episode 190: Total Reward = -61.80, Epsilon = 0.563\n",
      "Episode 200: Total Reward = -81.77, Epsilon = 0.547\n",
      "Episode 210: Total Reward = -63.95, Epsilon = 0.530\n",
      "Episode 220: Total Reward = -80.31, Epsilon = 0.515\n",
      "Episode 230: Total Reward = -95.27, Epsilon = 0.500\n",
      "Episode 240: Total Reward = -83.31, Epsilon = 0.485\n",
      "Episode 250: Total Reward = -80.28, Epsilon = 0.470\n",
      "Episode 260: Total Reward = -59.15, Epsilon = 0.456\n",
      "Episode 270: Total Reward = -70.27, Epsilon = 0.443\n",
      "Episode 280: Total Reward = -61.87, Epsilon = 0.430\n",
      "Episode 290: Total Reward = -56.89, Epsilon = 0.417\n",
      "Episode 300: Total Reward = -88.62, Epsilon = 0.405\n",
      "Episode 310: Total Reward = -90.92, Epsilon = 0.393\n",
      "Episode 320: Total Reward = -64.77, Epsilon = 0.381\n",
      "Episode 330: Total Reward = -74.26, Epsilon = 0.370\n",
      "Episode 340: Total Reward = -62.84, Epsilon = 0.359\n",
      "Episode 350: Total Reward = -67.65, Epsilon = 0.348\n",
      "Episode 360: Total Reward = -57.25, Epsilon = 0.338\n",
      "Episode 370: Total Reward = -59.29, Epsilon = 0.328\n",
      "Episode 380: Total Reward = -63.12, Epsilon = 0.318\n",
      "Episode 390: Total Reward = -70.43, Epsilon = 0.309\n",
      "Episode 400: Total Reward = -56.48, Epsilon = 0.300\n",
      "Episode 410: Total Reward = -54.99, Epsilon = 0.291\n",
      "Episode 420: Total Reward = -60.73, Epsilon = 0.282\n",
      "Episode 430: Total Reward = -63.73, Epsilon = 0.274\n",
      "Episode 440: Total Reward = -52.51, Epsilon = 0.266\n",
      "Episode 450: Total Reward = -55.66, Epsilon = 0.258\n",
      "Episode 460: Total Reward = -86.75, Epsilon = 0.250\n",
      "Episode 470: Total Reward = -54.08, Epsilon = 0.243\n",
      "Episode 480: Total Reward = -58.00, Epsilon = 0.236\n",
      "Episode 490: Total Reward = -42.30, Epsilon = 0.229\n",
      "Episode 500: Total Reward = -69.21, Epsilon = 0.222\n",
      "Episode 510: Total Reward = -54.49, Epsilon = 0.215\n",
      "Episode 520: Total Reward = -71.50, Epsilon = 0.209\n",
      "Episode 530: Total Reward = -15.43, Epsilon = 0.203\n",
      "Episode 540: Total Reward = -56.68, Epsilon = 0.197\n",
      "Episode 550: Total Reward = -35.07, Epsilon = 0.191\n",
      "Episode 560: Total Reward = 25.34, Epsilon = 0.185\n",
      "Episode 570: Total Reward = -57.32, Epsilon = 0.180\n",
      "Episode 580: Total Reward = -54.64, Epsilon = 0.175\n",
      "Episode 590: Total Reward = -93.74, Epsilon = 0.169\n",
      "Episode 600: Total Reward = -81.12, Epsilon = 0.164\n",
      "Episode 610: Total Reward = -61.42, Epsilon = 0.159\n",
      "Episode 620: Total Reward = -41.37, Epsilon = 0.155\n",
      "Episode 630: Total Reward = -22.34, Epsilon = 0.150\n",
      "Episode 640: Total Reward = -52.72, Epsilon = 0.146\n",
      "Episode 650: Total Reward = -77.59, Epsilon = 0.141\n",
      "Episode 660: Total Reward = -62.04, Epsilon = 0.137\n",
      "Episode 670: Total Reward = -54.43, Epsilon = 0.133\n",
      "Episode 680: Total Reward = -52.76, Epsilon = 0.129\n",
      "Episode 690: Total Reward = -52.93, Epsilon = 0.125\n",
      "Episode 700: Total Reward = -67.73, Epsilon = 0.122\n",
      "Episode 710: Total Reward = 9.78, Epsilon = 0.118\n",
      "Episode 720: Total Reward = 5.23, Epsilon = 0.115\n",
      "Episode 730: Total Reward = -53.74, Epsilon = 0.111\n",
      "Episode 740: Total Reward = 118.49, Epsilon = 0.108\n",
      "Episode 750: Total Reward = 15.56, Epsilon = 0.105\n",
      "Episode 760: Total Reward = -51.04, Epsilon = 0.102\n",
      "Episode 770: Total Reward = -52.87, Epsilon = 0.099\n",
      "Episode 780: Total Reward = -56.01, Epsilon = 0.096\n",
      "Episode 790: Total Reward = 19.17, Epsilon = 0.093\n",
      "Episode 800: Total Reward = -30.30, Epsilon = 0.090\n",
      "Episode 810: Total Reward = 9.62, Epsilon = 0.087\n",
      "Episode 820: Total Reward = -63.39, Epsilon = 0.085\n",
      "Episode 830: Total Reward = -50.95, Epsilon = 0.082\n",
      "Episode 840: Total Reward = -65.46, Epsilon = 0.080\n",
      "Episode 850: Total Reward = -67.27, Epsilon = 0.078\n",
      "Episode 860: Total Reward = -50.62, Epsilon = 0.075\n",
      "Episode 870: Total Reward = -76.90, Epsilon = 0.073\n",
      "Episode 880: Total Reward = -56.55, Epsilon = 0.071\n",
      "Episode 890: Total Reward = 83.25, Epsilon = 0.069\n",
      "Episode 900: Total Reward = -50.92, Epsilon = 0.067\n",
      "Episode 910: Total Reward = -45.90, Epsilon = 0.065\n",
      "Episode 920: Total Reward = 26.05, Epsilon = 0.063\n",
      "Episode 930: Total Reward = -47.85, Epsilon = 0.061\n",
      "Episode 940: Total Reward = -71.72, Epsilon = 0.059\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Game Settings ===\n",
    "WIDTH, HEIGHT = 288, 512\n",
    "PIPE_GAP = 100\n",
    "BIRD_X = 50\n",
    "FPS = 30\n",
    "\n",
    "# === DQN Settings ===\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "MEMORY_SIZE = 5000\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.997\n",
    "TARGET_UPDATE = 50\n",
    "\n",
    "# === Initialize Pygame ===\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# === Game Environment ===\n",
    "class FlappyBirdEnv:\n",
    "    def __init__(self, screen, clock):\n",
    "        self.screen = screen\n",
    "        self.clock = clock\n",
    "        self.frame_count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.bird_y = HEIGHT // 2\n",
    "        self.bird_vel = 0\n",
    "        self.pipe_x = WIDTH\n",
    "        self.pipe_top = random.randint(50, HEIGHT - PIPE_GAP - 50)\n",
    "        self.done = False\n",
    "        self.score = 0\n",
    "        self.frame_count = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        bird_mid = self.bird_y + 15\n",
    "        pipe_center = self.pipe_top + PIPE_GAP / 2\n",
    "        return np.array([\n",
    "            bird_mid / HEIGHT,\n",
    "            self.bird_vel / 10.0,\n",
    "            (self.pipe_x - BIRD_X) / WIDTH,\n",
    "            (pipe_center - bird_mid) / HEIGHT\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.bird_vel += 1\n",
    "        if action == 1:\n",
    "            self.bird_vel = -8\n",
    "        self.bird_y += self.bird_vel\n",
    "        self.pipe_x -= 4\n",
    "        self.frame_count += 1\n",
    "\n",
    "        pipe_center = self.pipe_top + PIPE_GAP / 2\n",
    "        distance_from_center = abs((self.bird_y + 15) - pipe_center)\n",
    "        reward = 0.1 + (1.0 - (distance_from_center / (HEIGHT / 2)))\n",
    "\n",
    "        if self.pipe_x < -50:\n",
    "            self.pipe_x = WIDTH\n",
    "            self.pipe_top = random.randint(50, HEIGHT - PIPE_GAP - 50)\n",
    "            self.score += 1\n",
    "            reward += 1\n",
    "\n",
    "        if (self.bird_y > HEIGHT or self.bird_y < 0 or\n",
    "           (self.pipe_x < BIRD_X + 30 < self.pipe_x + 50 and\n",
    "            (self.bird_y < self.pipe_top or self.bird_y > self.pipe_top + PIPE_GAP))):\n",
    "            reward = -100\n",
    "            self.done = True\n",
    "\n",
    "        if self.frame_count > 500:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_state(), reward, self.done\n",
    "\n",
    "    def render(self):\n",
    "        self.screen.fill((135, 206, 250))\n",
    "        pygame.draw.rect(self.screen, (255, 255, 0), (BIRD_X, self.bird_y, 30, 30))\n",
    "        pygame.draw.rect(self.screen, (0, 255, 0), (self.pipe_x, 0, 50, self.pipe_top))\n",
    "        pygame.draw.rect(self.screen, (0, 255, 0), (self.pipe_x, self.pipe_top + PIPE_GAP, 50, HEIGHT))\n",
    "\n",
    "        # Debug: draw center lines\n",
    "        pygame.draw.line(self.screen, (255, 0, 0), (0, self.bird_y + 15), (WIDTH, self.bird_y + 15), 1)\n",
    "        pipe_center = self.pipe_top + PIPE_GAP // 2\n",
    "        pygame.draw.line(self.screen, (0, 0, 255), (0, pipe_center), (WIDTH, pipe_center), 1)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(FPS)\n",
    "\n",
    "# === DQN Model ===\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Helper Functions ===\n",
    "def select_action(state, policy_net, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(torch.FloatTensor(state)).argmax().item()\n",
    "\n",
    "def train_step(policy_net, target_net, memory, optimizer):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "    rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.BoolTensor(dones).unsqueeze(1)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions)\n",
    "    with torch.no_grad():\n",
    "        q_next = target_net(next_states).max(1, keepdim=True)[0]\n",
    "        q_target = rewards + GAMMA * q_next * (~dones)\n",
    "\n",
    "    loss = nn.MSELoss()(q_values, q_target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# === Main Training Loop ===\n",
    "NUM_EPISODES = 1600\n",
    "env = FlappyBirdEnv(screen, clock)\n",
    "policy_net = DQN(4, 2)\n",
    "target_net = DQN(4, 2)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "epsilon = EPSILON_START\n",
    "rewards_list = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, policy_net, epsilon)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        train_step(policy_net, target_net, memory, optimizer)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if episode % 200 == 0:\n",
    "            env.render()\n",
    "\n",
    "    epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
    "    rewards_list.append(total_reward)\n",
    "\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}: Total Reward = {total_reward:.2f}, Epsilon = {epsilon:.3f}\")\n",
    "\n",
    "# === Visualization ===\n",
    "plt.plot(rewards_list)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Agent Learning Curve')\n",
    "plt.show()\n",
    "\n",
    "# === Agent Playback ===\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "clock = pygame.time.Clock()\n",
    "env = FlappyBirdEnv(screen, clock)\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "running = True\n",
    "\n",
    "while not done and running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "            done = True\n",
    "\n",
    "    env.render()\n",
    "    action = select_action(state, policy_net, epsilon=0.0)\n",
    "    next_state, _, done = env.step(action)\n",
    "    state = next_state\n",
    "    pygame.time.wait(10)\n",
    "\n",
    "pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db296ce-eb70-49b6-8b73-5e64fd15f339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91fda5-7a67-4b22-b52e-51781a7d9293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44acdac-d066-43c6-aa26-c278ba77f1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
